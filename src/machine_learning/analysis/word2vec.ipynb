{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import KeyedVectors \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient(\"mongodb+srv://adamstuller:Mit29kis@cluster0-rnyqh.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "db = client.bpdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_pos_set = set(['D', 'A', 'S'])\n",
    "\n",
    "def only_allowed_pos(x):\n",
    "    try:\n",
    "        return True if x['pos'][0] in allowed_pos_set else False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def filter_allowed_pos(comment):\n",
    "    return list(filter(only_allowed_pos,comment))\n",
    "\n",
    "def concat_words(words):\n",
    "    lemmas = [w['lemma'][0] for w in words]\n",
    "    lemmas = list(filter(lambda x: x!= '?', lemmas))\n",
    "    if len(lemmas) == 0:\n",
    "        return ''\n",
    "    else:\n",
    "        return \" \".join(lemmas)\n",
    "\n",
    "updated_df = pd.DataFrame([ [concat_words(filter_allowed_pos(x['processed'])), x['class']] for x in db.comments.find().limit(3000)], columns= ['sentence', 'class'])\n",
    "\n",
    "updated_df = updated_df[updated_df.sentence != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "updated_df['class'] = le.fit_transform(updated_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vector_dict = KeyedVectors.load_word2vec_format('./data/vec-sk-skipgram-lemma', binary=True)\n",
    "        self.D = len(self.vector_dict.get_vector('predaj≈àa'))\n",
    "        \n",
    "    def fit(self, data):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \n",
    "        def tranform_sentence_to_vector(sentence):\n",
    "            vectors = []\n",
    "            for word in sentence['sentence']:\n",
    "                try:\n",
    "                    vectors.append(self.vector_dict.get_vector(word))\n",
    "                except:\n",
    "                    pass\n",
    "            output = np.full(200,np.nan) if len(vectors) ==0 else np.array(vectors).mean(axis =0)\n",
    "            return  np.append( [*output], sentence['class'])\n",
    "        \n",
    "        return  data.apply(tranform_sentence_to_vector, axis=1)\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = Word2VecVectorizer()\n",
    "df = pd.DataFrame([*vect.transform(updated_df)]).rename(columns = lambda x: 'class' if x == 200 else x).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test( df ):\n",
    "    x_trains, x_tests,  y_trains, y_tests = [], [], [], []\n",
    "    for tag in df['class'].unique().tolist():\n",
    "        x, y = df[df['class'] == tag].drop(columns='class'), df[df['class'] == tag]['class']\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=1000)\n",
    "        x_trains.append(x_train)\n",
    "        x_tests.append(x_test)    \n",
    "        y_trains.append(y_train)    \n",
    "        y_tests.append(y_test)   \n",
    "\n",
    "    return  pd.concat(x_trains), pd.concat(x_tests), pd.concat(y_trains), pd.concat(y_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "        \n",
    "    \n",
    "    \n",
    "neural_network = MLPClassifier(hidden_layer_sizes=(50,), max_iter=100, alpha=1e-4,\n",
    "                                   solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                                   learning_rate_init=.1)\n",
    "classifier1 = LogisticRegression(solver='lbfgs')\n",
    "classifier2 = GaussianNB()\n",
    "classifier3 = MultinomialNB()\n",
    "classifier4 = SGDClassifier()\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=60,random_state=42)\n",
    "support_vector_machine = svm.SVC(gamma='scale')\n",
    "    \n",
    "def train_classifier( x_train, x_test, y_train, y_test, classifier):\n",
    "    classifier.fit(x_train, y_train)\n",
    "    return classifier, classifier.score(x_test, y_test)\n",
    "\n",
    "def predict( model , messages, vectorizer):\n",
    "    message_series = pd.Series(messages)\n",
    "    return model.predict(vectorizer.transform(message_series))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.90281393\n",
      "Iteration 2, loss = 1.73837652\n",
      "Iteration 3, loss = 1.72718430\n",
      "Iteration 4, loss = 1.70742675\n",
      "Iteration 5, loss = 1.69156516\n",
      "Iteration 6, loss = 1.68301102\n",
      "Iteration 7, loss = 1.66960139\n",
      "Iteration 8, loss = 1.65953813\n",
      "Iteration 9, loss = 1.64145166\n",
      "Iteration 10, loss = 1.62725207\n",
      "Iteration 11, loss = 1.61932111\n",
      "Iteration 12, loss = 1.60863044\n",
      "Iteration 13, loss = 1.59903410\n",
      "Iteration 14, loss = 1.58378304\n",
      "Iteration 15, loss = 1.58972259\n",
      "Iteration 16, loss = 1.57597988\n",
      "Iteration 17, loss = 1.56627165\n",
      "Iteration 18, loss = 1.56379569\n",
      "Iteration 19, loss = 1.55382588\n",
      "Iteration 20, loss = 1.55389074\n",
      "Iteration 21, loss = 1.54657101\n",
      "Iteration 22, loss = 1.53850450\n",
      "Iteration 23, loss = 1.53573223\n",
      "Iteration 24, loss = 1.53030898\n",
      "Iteration 25, loss = 1.54176755\n",
      "Iteration 26, loss = 1.52160654\n",
      "Iteration 27, loss = 1.51601590\n",
      "Iteration 28, loss = 1.51413623\n",
      "Iteration 29, loss = 1.50882200\n",
      "Iteration 30, loss = 1.50415145\n",
      "Iteration 31, loss = 1.51064941\n",
      "Iteration 32, loss = 1.51056377\n",
      "Iteration 33, loss = 1.49308823\n",
      "Iteration 34, loss = 1.51094045\n",
      "Iteration 35, loss = 1.48822593\n",
      "Iteration 36, loss = 1.51128651\n",
      "Iteration 37, loss = 1.48357551\n",
      "Iteration 38, loss = 1.49283345\n",
      "Iteration 39, loss = 1.49837151\n",
      "Iteration 40, loss = 1.46808867\n",
      "Iteration 41, loss = 1.47369826\n",
      "Iteration 42, loss = 1.48531535\n",
      "Iteration 43, loss = 1.47780510\n",
      "Iteration 44, loss = 1.48635400\n",
      "Iteration 45, loss = 1.47428997\n",
      "Iteration 46, loss = 1.46555891\n",
      "Iteration 47, loss = 1.46375921\n",
      "Iteration 48, loss = 1.44960355\n",
      "Iteration 49, loss = 1.45329269\n",
      "Iteration 50, loss = 1.49931368\n",
      "Iteration 51, loss = 1.44929094\n",
      "Iteration 52, loss = 1.44939797\n",
      "Iteration 53, loss = 1.45089464\n",
      "Iteration 54, loss = 1.44526859\n",
      "Iteration 55, loss = 1.44120431\n",
      "Iteration 56, loss = 1.46070349\n",
      "Iteration 57, loss = 1.44158335\n",
      "Iteration 58, loss = 1.43240478\n",
      "Iteration 59, loss = 1.44163878\n",
      "Iteration 60, loss = 1.45079512\n",
      "Iteration 61, loss = 1.44155594\n",
      "Iteration 62, loss = 1.44445759\n",
      "Iteration 63, loss = 1.44540541\n",
      "Iteration 64, loss = 1.47560415\n",
      "Iteration 65, loss = 1.42040028\n",
      "Iteration 66, loss = 1.41981701\n",
      "Iteration 67, loss = 1.44492858\n",
      "Iteration 68, loss = 1.42321973\n",
      "Iteration 69, loss = 1.41026650\n",
      "Iteration 70, loss = 1.40484425\n",
      "Iteration 71, loss = 1.41639867\n",
      "Iteration 72, loss = 1.40497424\n",
      "Iteration 73, loss = 1.44050011\n",
      "Iteration 74, loss = 1.40748903\n",
      "Iteration 75, loss = 1.40803464\n",
      "Iteration 76, loss = 1.42197375\n",
      "Iteration 77, loss = 1.40695858\n",
      "Iteration 78, loss = 1.42088866\n",
      "Iteration 79, loss = 1.40659940\n",
      "Iteration 80, loss = 1.41448773\n",
      "Iteration 81, loss = 1.40767554\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "model, score = train_classifier( *get_train_test(df) , neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47183098591549294"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict = KeyedVectors.load_word2vec_format('./data/vec-sk-skipgram-lemma', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Prosiiim' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-48ba81f49241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvector_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prosiiim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Prosiiim' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "vector_dict.get_vector('Prosiiim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
